# PRD: Self-Learning Pattern System for CONSTRUCT

## Overview
Introduce a self-learning mechanism to CONSTRUCT's pattern system, enabling automatic detection of improvement opportunities in patterns, validators, and documentation. This feature analyzes usage data from logs, git history, and validation results to suggest refinements, new patterns, or optimizations. It aligns with CONSTRUCT's philosophy of "self-improving patterns" by turning real-world development workflows into iterative enhancements, reducing manual intervention while keeping humans in the loop for approvals.

## Problem Statement
Currently, patterns in CONSTRUCT evolve manually: Developers edit .md/.yaml files in LAB, test via validators, and share as repositories. This works but misses opportunities to learn from usage—e.g., repeated validator failures indicate weak rules, or git diffs show emerging code structures not covered by existing patterns. Without automated feedback loops, patterns can become outdated, leading to friction in AI-assisted development (e.g., frequent manual refinements during sessions). This contradicts the dynamic context engineering vision, where the system should adapt based on what developers are "actually doing."

## Solution
Add a learning layer that collects data passively during normal workflows (e.g., validations, commits), analyzes for patterns/gaps, and suggests actionable changes (e.g., .md diffs, new plugins). Suggestions land in LAB for review, testing via existing validators, and optional application. This creates closed loops: Detect → Suggest → Apply → Measure Impact.

## Strategic Alignment
- **Self-Improving Patterns**: Directly realizes the README's "Pattern Learning" by identifying repeated structures and suggesting evolutions.
- **Dynamic Context**: Enhances context assembly (e.g., auto-suggest plugins based on file types).
- **Unix Philosophy**: Builds on simple text logs/diffs; no heavy dependencies initially.
- **Community Scalable**: Learned patterns can migrate from LAB to CORE/shared repos.

## Key Features
- **Data Collection**: Extend logs with pattern-specific metrics (e.g., fails per plugin).
- **Analysis & Suggestions**: Command to detect issues (e.g., high-fail patterns) and propose fixes.
- **Analytics Reporting**: Summarize trends across projects/workspaces.
- **Integration**: Hook into pre-commit, workspace tools for seamless loops.
- **Docs Tie-In**: Support learning for .docs.md (e.g., underused sections).

## Requirements
- **Minimal Dependencies**: Bash/yq/git; optional Python in lib/ for advanced analysis.
- **Project-Aware**: Works on single PROJECT_DIR or workspace-wide.
- **Configurable**: Enable/disable via .construct/patterns.yaml (e.g., learning: enabled).
- **Secure**: No external data sharing; all local.

## Success Criteria & Metrics
| Metric | Target | Measurement |
|--------|--------|-------------|
| Suggestion Accuracy | >70% useful (user-adopted) | Track adopted vs. generated in logs |
| Performance Overhead | <5% added time to checks | Time validators with/without logging |
| Reduction in Manual Edits | 30% fewer pattern tweaks | Compare git commits to patterns/ pre/post |
| Coverage | Detect 80% of repeat fails | Manual review of logs vs. suggestions |
| Adoption | Used in 50% of commits | Analytics from learning-logs/ |

Qualitative: Developers report fewer "explaining patterns" in AI sessions; patterns feel more "laser-focused."

## Implementation Details

### Directory Changes
```
CONSTRUCT-CORE/
├── patterns/
│   └── lib/
│       └── learn-utils.sh  # Shared functions (e.g., parse_logs)
├── scripts/
│   ├── core/
│   │   └── learn-patterns.sh  # Main learning command
│   └── dev/
│       └── pattern-analytics.sh  # Reporting tool
Projects/MyApp/
└── .construct/
    ├── patterns.yaml  # Add learning: {enabled: true, threshold: 5}
    └── learning-logs/  # New: pattern_usage.log, validator_fails.log
CONSTRUCT-LAB/patterns/
└── experiments/  # Suggestions land here as diffs/stubs
```

### Learning Configuration in patterns.yaml
```yaml
learning:
  enabled: true
  fail_threshold: 5  # Suggest if >5 fails per pattern
  git_analyze_depth: 100  # Commits to scan
  scopes: [all, docs]  # Limit to docs or full patterns
```

### Data Collection
Extend existing scripts:
- In validators (e.g., quality.sh): After checks, log:
  ```bash
  echo "$(date): pattern:$PLUGIN fail:$ISSUE_TYPE count:$ISSUES file:$FILE" >> "$PROJECT_DIR/.construct/learning-logs/validator_fails.log"
  ```
- In assemble-claude.sh: Log loaded patterns:
  ```bash
  echo "$(date): loaded:$plugin sections:$(grep '^#' "$plugin_path" | wc -l)" >> "$PROJECT_DIR/.construct/learning-logs/pattern_usage.log"
  ```
- In pre-commit-review.sh: Aggregate script results into learning-logs/summary.log.

### Analysis Logic
- **New Patterns**: Scan git for repeats (e.g., regex for "class *ViewModel").
- **Refinements**: Group fails by type (e.g., naming issues → suggest rule addition).
- **Docs-Specific**: Count section accesses (e.g., via git diffs mentioning keywords).
- Use learn-utils.sh for common parsing (e.g., awk/grep on logs).

### Suggestion Format
- Outputs: Console summary + files in experiments/ (e.g., mvvm-ios-refine.diff, new-custom-ui.md).
- Example Diff:
  ```
  --- mvvm-ios.md
  +++ mvvm-ios.md (suggested)
  @@ -10,6 +10 @@
   ## New Rule: Avoid Force Unwraps
   - From logs: 7 instances in recent commits
   ```

## Step-by-Step Implementation Guide

### Phase 1: Setup Data Collection (Day 1-2)
1. **Create learning-logs/ Template**:
   - In create-project.sh/import-project.sh: Add `mkdir -p .construct/learning-logs`.
   
2. **Add Logging to Validators**:
   - For each validator type (quality.sh, etc.) in plugins/*/validators/:
     - Include learn-utils.sh: `source "$CONSTRUCT_CORE/patterns/lib/learn-utils.sh"`.
     - After issue detection: `log_fail "$PROJECT_DIR" "$PLUGIN" "$ISSUE_TYPE" "$FILE"`.
   - Implement log_fail in learn-utils.sh:
     ```bash
     log_fail() {
         local dir=$1 pattern=$2 type=$3 file=$4
         echo "$(date): pattern:$pattern fail:$type file:$file" >> "$dir/.construct/learning-logs/validator_fails.log"
     }
     ```

3. **Log Pattern Usage**:
   - In assemble-claude.sh, after adding plugin: `log_usage "$PROJECT_DIR" "$plugin" "$(wc -l < "$plugin_path")"`.
   
4. **Test**:
   - Create mock project: `./workspace/create-project.sh TestProj ios`.
   - Run `check-quality.sh TestProj`: Verify logs in .construct/learning-logs/.
   - Simulate fails: Edit code to trigger issues, re-run checks.

### Phase 2: Implement Learning Command (Day 3-4)
1. **Create core/learn-patterns.sh**:
   - Accept PROJECT_DIR, --scope, --apply.
   - Parse Logs: Use awk to group fails (e.g., `awk '{print $3}' validator_fails.log | sort | uniq -c`).
   - Git Analysis: `git log -p --since=30.days | grep -c "ViewModel"`.
   - Generate Suggestions: If threshold met, create diff/stub in LAB/patterns/experiments/.
   
2. **Handle Docs**: If scope=docs, focus on .docs.md (e.g., underused sections via keyword grep in git diffs).

3. **Test**:
   - Run `./core/learn-patterns.sh TestProj`: Expect suggestions if logs have >threshold fails.
   - Verify files in experiments/: Apply manually, regenerate CLAUDE.md, re-validate.
   - Edge Case: Empty logs → "No suggestions."
   - Multi-Project: `./workspace/workspace-update.sh --learn` (extend script to chain learn-patterns).

### Phase 3: Analytics & Reporting (Day 5)
1. **Create dev/pattern-analytics.sh**:
   - Aggregate logs: Table of pattern fails/usage.
   - Example Output:
     | Pattern | Loads | Fails | Suggestion |
     |---------|-------|-------|------------|
     | swift | 50 | 15 | Refine naming rules |

2. **Integrate with Pre-Commit**:
   - In pre-commit-review.sh, after summary: If FAILED>0, `learn-patterns.sh "$PROJECT_DIR" --suggest-only`.

3. **Test**:
   - Generate reports: `./dev/pattern-analytics.sh TestProj`.
   - Commit Flow: Trigger pre-commit, see suggestions on fails.
   - Measure: Simulate 10 fails, adopt suggestion, re-run: Verify reduced fails in logs.

### Phase 4: Feedback Loops & Optimization (Day 6-7)
1. **Post-Apply Tracking**:
   - In learn-patterns.sh --apply: Re-run validators, log "pre_fail: X post_fail: Y".
   
2. **Workspace Integration**:
   - In workspace-status.sh: Add learning summary (e.g., "3 suggestions pending in LAB").

3. **Test & Validate**:
   - Full Workflow: Create project, code/edit (trigger fails), pre-commit (see suggestions), apply, commit.
   - Performance: Time checks with/without learning (<5% overhead).
   - Scenarios:
     - New Pattern: Add repeated code in git, learn detects.
     - Refinement: Force fails, learn suggests, apply reduces.
     - Docs: If .docs.md exists, simulate underuse (low mentions), suggest expansion.
   - Cross-Project: Import 2 projects, run workspace-update --learn: Aggregated suggestions.

### Phase 5: Polish & Future (Week 2+)
- **Config Options**: Add to patterns.yaml.
- **AI Enhancement**: If code_execution tool available, use for advanced parsing (e.g., Python sympy for math patterns).
- **Automation**: Cron-like for periodic learn (e.g., in CI).
- **Validation**: Run existing checks on suggested files; alert if they fail.

## Rollout Plan
- **Week 1**: Phases 1-3 in LAB; test on mock projects.
- **Week 2**: Phase 4-5; deploy to real iOS/Watch projects.
- **Week 3**: Gather feedback (e.g., via dev-updates logs), refine thresholds.
- **Week 4**: Document in READMEs (e.g., patterns/README.md), add --help to new scripts.

## Open Questions
1. **Threshold Tuning**: Start at 5 fails—user-configurable?
   - **Rec**: Yes, via YAML.
2. **AI Integration**: Use code_execution for analysis?
   - **Rec**: Optional in Phase 5.
3. **Scope Limits**: Start with validators/docs; expand to generators?
   - **Rec**: Phase 4+.

## Summary
This PRD adds self-learning to CONSTRUCT, making patterns evolve from usage data. Implementation is phased, leveraging existing logs/scripts for minimal disruption. Expected Impact: 30% fewer manual edits, more adaptive contexts, accelerating product shipping.